groups:
  - name: application_alerts
    interval: 30s
    rules:
      # High Error Rate Alert
      - alert: HighErrorRate
        expr: |
          rate(http_server_requests_seconds_count{application="retail-backend",status=~"5.."}[5m])
          /
          rate(http_server_requests_seconds_count{application="retail-backend"}[5m])
          > 0.05
        for: 5m
        labels:
          severity: critical
          component: application
        annotations:
          summary: "High HTTP error rate detected"
          description: "Error rate is {{ $value | humanizePercentage }} (threshold: 5%)"
          dashboard: "https://grafana.example.com/d/retail-app-overview"

      # Slow Response Time Alert
      - alert: SlowResponseTime
        expr: |
          histogram_quantile(0.95,
            rate(http_server_requests_seconds_bucket{application="retail-backend"}[5m])
          ) > 1
        for: 10m
        labels:
          severity: warning
          component: application
        annotations:
          summary: "Slow API response time detected"
          description: "95th percentile response time is {{ $value }}s (threshold: 1s)"
          dashboard: "https://grafana.example.com/d/retail-app-overview"

      # Order Processing Failure Alert
      - alert: HighOrderFailureRate
        expr: |
          rate(orders_failed_total{application="retail-backend"}[5m])
          /
          rate(orders_created_total{application="retail-backend"}[5m])
          > 0.1
        for: 5m
        labels:
          severity: critical
          component: orders
        annotations:
          summary: "High order processing failure rate"
          description: "Order failure rate is {{ $value | humanizePercentage }} (threshold: 10%)"
          dashboard: "https://grafana.example.com/d/retail-app-overview"

      # Payment Processing Failure Alert
      - alert: HighPaymentFailureRate
        expr: |
          rate(payments_failed_total{application="retail-backend"}[5m])
          /
          rate(payments_processed_total{application="retail-backend"}[5m])
          > 0.2
        for: 5m
        labels:
          severity: critical
          component: payments
        annotations:
          summary: "High payment processing failure rate"
          description: "Payment failure rate is {{ $value | humanizePercentage }} (threshold: 20%)"
          dashboard: "https://grafana.example.com/d/retail-app-overview"

      # Low Cache Hit Rate Alert
      - alert: LowCacheHitRate
        expr: |
          sum(rate(cache_hits_total{application="retail-backend"}[5m]))
          /
          (sum(rate(cache_hits_total{application="retail-backend"}[5m])) +
           sum(rate(cache_misses_total{application="retail-backend"}[5m])))
          < 0.5
        for: 10m
        labels:
          severity: warning
          component: cache
        annotations:
          summary: "Low cache hit rate"
          description: "Cache hit rate is {{ $value | humanizePercentage }} (threshold: 50%)"
          dashboard: "https://grafana.example.com/d/retail-app-overview"

  - name: infrastructure_alerts
    interval: 30s
    rules:
      # High Memory Usage Alert
      - alert: HighMemoryUsage
        expr: |
          jvm_memory_used_bytes{application="retail-backend",area="heap"}
          /
          jvm_memory_max_bytes{application="retail-backend",area="heap"}
          > 0.9
        for: 5m
        labels:
          severity: warning
          component: jvm
        annotations:
          summary: "High JVM heap memory usage"
          description: "Heap memory usage is {{ $value | humanizePercentage }} (threshold: 90%)"
          dashboard: "https://grafana.example.com/d/retail-infrastructure"

      # Critical Memory Usage Alert
      - alert: CriticalMemoryUsage
        expr: |
          jvm_memory_used_bytes{application="retail-backend",area="heap"}
          /
          jvm_memory_max_bytes{application="retail-backend",area="heap"}
          > 0.95
        for: 2m
        labels:
          severity: critical
          component: jvm
        annotations:
          summary: "Critical JVM heap memory usage"
          description: "Heap memory usage is {{ $value | humanizePercentage }} (threshold: 95%)"
          dashboard: "https://grafana.example.com/d/retail-infrastructure"

      # Excessive GC Time Alert
      - alert: ExcessiveGCTime
        expr: |
          rate(jvm_gc_pause_seconds_sum{application="retail-backend"}[1m]) > 0.1
        for: 5m
        labels:
          severity: warning
          component: jvm
        annotations:
          summary: "Excessive garbage collection time"
          description: "GC is taking {{ $value | humanizePercentage }} of CPU time (threshold: 10%)"
          dashboard: "https://grafana.example.com/d/retail-infrastructure"

      # Thread Exhaustion Alert
      - alert: HighThreadCount
        expr: |
          jvm_threads_live_threads{application="retail-backend"} > 500
        for: 10m
        labels:
          severity: warning
          component: jvm
        annotations:
          summary: "High JVM thread count"
          description: "Thread count is {{ $value }} (threshold: 500)"
          dashboard: "https://grafana.example.com/d/retail-infrastructure"

  - name: database_alerts
    interval: 30s
    rules:
      # MongoDB Connection Pool Exhaustion Alert
      - alert: MongoDBConnectionPoolExhaustion
        expr: |
          mongodb_driver_pool_checkedout{application="retail-backend"}
          /
          mongodb_driver_pool_size{application="retail-backend"}
          > 0.8
        for: 5m
        labels:
          severity: warning
          component: mongodb
        annotations:
          summary: "MongoDB connection pool nearly exhausted"
          description: "{{ $value | humanizePercentage }} of connections in use (threshold: 80%)"
          dashboard: "https://grafana.example.com/d/retail-infrastructure"

      # Redis High Latency Alert
      - alert: RedisHighLatency
        expr: |
          histogram_quantile(0.95,
            rate(lettuce_command_latency_seconds_bucket{application="retail-backend"}[5m])
          ) > 0.1
        for: 10m
        labels:
          severity: warning
          component: redis
        annotations:
          summary: "High Redis command latency"
          description: "Redis P95 latency is {{ $value }}s (threshold: 100ms)"
          dashboard: "https://grafana.example.com/d/retail-infrastructure"

  - name: availability_alerts
    interval: 30s
    rules:
      # Service Down Alert
      - alert: ServiceDown
        expr: |
          up{job="retail-backend"} == 0
        for: 1m
        labels:
          severity: critical
          component: application
        annotations:
          summary: "Retail backend service is down"
          description: "Service {{ $labels.instance }} is not responding"

      # MongoDB Down Alert
      - alert: MongoDBDown
        expr: |
          mongodb_up{application="retail-backend"} == 0
        for: 2m
        labels:
          severity: critical
          component: mongodb
        annotations:
          summary: "MongoDB database is down"
          description: "MongoDB connection is unavailable"

      # Redis Down Alert
      - alert: RedisDown
        expr: |
          redis_up{application="retail-backend"} == 0
        for: 2m
        labels:
          severity: critical
          component: redis
        annotations:
          summary: "Redis cache is down"
          description: "Redis connection is unavailable"

      # Elasticsearch Down Alert
      - alert: ElasticsearchDown
        expr: |
          elasticsearch_up{application="retail-backend"} == 0
        for: 2m
        labels:
          severity: critical
          component: elasticsearch
        annotations:
          summary: "Elasticsearch is down"
          description: "Elasticsearch connection is unavailable"

  - name: business_alerts
    interval: 30s
    rules:
      # Low Order Volume Alert (potential issue)
      - alert: LowOrderVolume
        expr: |
          rate(orders_created_total{application="retail-backend"}[30m]) * 3600 < 10
        for: 1h
        labels:
          severity: warning
          component: business
        annotations:
          summary: "Unusually low order volume"
          description: "Order creation rate is {{ $value }} orders/hour (expected: >10/hour)"
          dashboard: "https://grafana.example.com/d/retail-app-overview"

      # Authentication Failure Spike Alert
      - alert: HighAuthenticationFailureRate
        expr: |
          rate(auth_login_failures_total{application="retail-backend"}[5m])
          /
          rate(auth_login_attempts_total{application="retail-backend"}[5m])
          > 0.3
        for: 10m
        labels:
          severity: warning
          component: authentication
        annotations:
          summary: "High authentication failure rate"
          description: "Login failure rate is {{ $value | humanizePercentage }} (threshold: 30%)"
          dashboard: "https://grafana.example.com/d/retail-app-overview"
